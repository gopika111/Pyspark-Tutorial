# Pyspark-Tutorial

To resolve the issue of big data, hadoop was released.
Hadoop had three main parts -> HDFS (Hadoop Distributed File System), Map Reduce (the computation engine) and YARN (Yet Another Resource Negotiator)
Working of Hadoop -> There was a Master Node and multiple Slave Nodes. The process requests comes directly to the Master Node and is splitted across Slave Nodes for processing.
Later on Apache Spark was released. In Apache Spark, the computation engine is Spark.Storage can be Local/HDFS/Amazon S3 and Resource Manager can be YARN/MESOS/KUBERNETES.
