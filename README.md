# Pyspark-Tutorial<br/>

To resolve the issue of **Big Data**, **Hadoop** was released.<br/>
**Hadoop** had three main parts -> **HDFS (Hadoop Distributed File System), Map Reduce (the computation engine) and YARN (Yet Another Resource Negotiator)**<br/>
Working of Hadoop -> There was a Master Node and multiple Slave Nodes. The process requests comes directly to the Master Node and is splitted across Slave Nodes for processing.<br/>
Later on Apache Spark was released. In **Apache Spark**, the computation engine is **Spark**.**Storage** can be **Local/HDFS/Amazon S3** and **Resource Manager** can be **YARN/MESOS/KUBERNETES**.
