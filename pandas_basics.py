"""Spark is a computation engine.
PySpark is a Python API used to leverage Spark features to tame big data.
Using PySpark we can work with RDD in Python.

About Resilient Distributed Dataset (RDD):
RDD is the fundamental unit of PySpark.

Working : While working on a big data, it gets converted to RDD and then for further processing of data
the master node partitions the RDD and processes it with the help of slave nodes.
"""




